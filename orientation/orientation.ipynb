{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *corpkit*: a Python-based toolkit for working with parsed linguistic corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Daniel McDonald](mailto:mcdonaldd@unimelb.edu.au?Subject=corpkit)**\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "> **SUMMARY:** This *IPython Notebook* shows you how to use `corpkit` to investigate a corpus of paragraphs containing the word *risk* in the NYT between 1963 and 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the functions we'll be using to investigate the corpus. These functions are designed for this interrogation, but also have more general use in mind, so you can likely use them on your own corpora.\n",
    "\n",
    "| **Function name** | Purpose                            | |\n",
    "| ----------------- | ---------------------------------- | |\n",
    "| `interrogator()`  | interrogate parsed corpora         | |\n",
    "| `dependencies()`  | interrogate parsed corpora for dependency info (presented later)         | |\n",
    "| `plotter()`       | visualise `interrogator()` results | |\n",
    "| `table()`          | return plotter() results as table | |\n",
    "| `quickview()`     | view `interrogator()` results      | |\n",
    "| `tally()`       | get total frequencies for `interrogator()` results      | |\n",
    "| `surgeon()`       | edit `interrogator()` results      | |\n",
    "| `merger()`       | merge `interrogator()` results      | |\n",
    "| `conc()`          | complex concordancing of subcorpora | |\n",
    "| `keywords()`          | get keywords and ngrams from `conc()` output | |\n",
    "| `collocates()`          | get collocates from `conc()` output| |\n",
    "| `quicktree()`          | visually represent a parse tree | |\n",
    "| `searchtree()`          | search a parse tree with a Tregex query | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import corpkit\n",
    "from corpkit import interrogator, plotter\n",
    "# show visualisations inline:\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's set the path to our corpus. If you were using this interface for your own corpora, you would change this to the path to your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to unzip nyt files:\n",
    "# gzip -dc data/nyt.tar.gz | tar -xf - -C data\n",
    "# corpus with annual subcorpora\n",
    "annual_trees = 'data/nyt/years' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main corpus is comprised of paragraphs from *New York Times* articles that contain a risk word, which we have defined by regular expression as '(?i)'.?\\brisk.?\\b'. This includes *low-risk*, or *risk/reward* as single tokens, but excludes *brisk* or *asterisk*.\n",
    "\n",
    "The data comes from a number of sources.\n",
    "\n",
    "* 1963 editions were downloaded from ProQuest Newsstand as PDFs. Optical character recognition and manual processing was used to create a set of 1200 risk sentences.\n",
    "* The 1987--2006 editions were taken from the *NYT Annotated Corpus*.\n",
    "* 2007--2014 editions were downloaded from *ProQuest Newsstand* as HTML.\n",
    "\n",
    "In total, 149,504 documents were processed. The corpus from which the risk corpus was made is over 150 million words in length!\n",
    "\n",
    "The texts have been parsed for part of speech and grammatical structure by [`Stanford CoreNLP*](http://nlp.stanford.edu/software/corenlp.shtml). In this Notebook, we are only working with the parsed versions of the texts. We rely on [*Tregex*](http://nlp.stanford.edu/~manning/courses/ling289/Tregex.html) to interrogate the corpora. Tregex allows very complex searching of parsed trees, in combination with [Java Regular Expressions](http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html). It's definitely worthwhile to learn the Tregex syntax, but in case you're time-poor, at the end of this notebook are a series of Tregex queries that you can copy and paste into *interrogator()` and `conc()` queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interrogating the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's start by generating some general information about this corpus. First, let's define a query to find every word in the corpus. Run the cell below to define the `allwords_query` variable as the Tregex query to its right.\n",
    "\n",
    "> *When writing Tregex queries or Regular Expressions, remember to always use `r'...'` quotes!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# any token containing letters or numbers (i.e. no punctuation):\n",
    "allwords_query = r'/[A-Za-z0-9]/ !< __' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform interrogations with `interrogator()`. Its most important arguments are:\n",
    "\n",
    "1. **path to corpus**\n",
    "\n",
    "2. Tregex **options**:\n",
    "  * **'-t'**: return only words\n",
    "  * **'-C'**: return a count of matches\n",
    "  * **'-u'**: return only the tag\n",
    "  * **'-o'**: return tag and word together\n",
    "\n",
    "3. the **Tregex query**\n",
    "\n",
    "We only need to count tokens, so we can use the **-C** option (it's often faster than getting lists of matching tokens). The cell below will run `interrogator()` over each annual subcorpus and count the number of matches for the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allwords = interrogator(annual_trees, '-C', allwords_query) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the interrogation has finished, we can view our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from the allwords results, print the totals\n",
    "print allwords.totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see the query and options that created the results, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print allwords.query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists of years and totals are pretty dry. Luckily, we can use the `plotter()` function to visualise our results. At minimum, `plotter()` needs two arguments:\n",
    "\n",
    "1. a title (in quotation marks)\n",
    "2. a list of results to plot\n",
    "\n",
    "There is also an argument for projecting the 1963 and 2014 results, which can either be set to true or false. By default, it is true, and in this Notebook from now on we'll leave it turned on. We'll try both options here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotter('Word counts in each subcorpus', allwords.totals, projection = False)\n",
    "plotter('Word counts in each subcorpus (projected)', allwords.totals, projection = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So, we can see that the number of words per year varies quite a lot. That's worth keeping in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of risk words in the NYT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's count the total number of risk words. Notice that we are using the '-o' flag, instead of the **-C** flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# our query:\n",
    "riskwords_query = r'__ < /(?i).?\\brisk.?\\b/' # any risk word and its word class/part of speech\n",
    "# get all risk words and their tags:\n",
    "riskwords = interrogator(annual_trees, '-o', riskwords_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when do not use the `-C` flag, we can access the total number of matches as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotter('Risk words', riskwords.totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, it's hard to tell whether or not these counts are simply because our annual NYT samples are different sizes. To account for this, we can calculate the percentage of parsed words that are risk words. This means combining the two interrogations we have already performed.\n",
    "\n",
    "We can do this by passing a third argument to `plotter()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotter('Relative frequency of risk words', riskwords.totals, \n",
    "    fract_of = allwords.totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's more helpful. We can now see some interesting peaks and troughs in the proportion of risk words. We can also see that 1963 contains the highest proportion of risk words. This is because the manual corrector of 1963 OCR entries preserved only the sentence containing risk words, rather than the paragraph.\n",
    "\n",
    "It's often helpful to not plot 1963 results for this reason. To do this, we can add an argument to the `plotter()` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotter('Relative frequency of risk words', riskwords.totals, \n",
    "    fract_of = allwords.totals, skip63 = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we're interested in not only the frequency of risk words, but the frequency of different `kinds* of risk words. We actually already collected this data during our last *interrogator()` query.\n",
    "\n",
    "We can print just the first three entries of the results list, rather than the totals list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in riskwords.results[:3]:\n",
    "    print word\n",
    "# uncomment below to print the totals:\n",
    "# print riskwords.totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have enough data to do some serious plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotter('Risk word / all risk words', riskwords.results, \n",
    "    fract_of = riskwords.totals)\n",
    "plotter('Risk word / all words', riskwords.results, \n",
    "    fract_of = allwords.totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customising visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `plotter()` plots the seven most frequent results, including 1963 and projecting 1963 and 2014.\n",
    "\n",
    " We can use other `plotter()` arguments to customise what our chart shows. `plotter()`'s possible arguments are:\n",
    "\n",
    " | plotter() argument | Mandatory/default?       |  Use          | Type  |\n",
    " | :------|:------- |:-------------|:-----|\n",
    " | *title* | **mandatory**      | A title for your plot | string |\n",
    " | `results* | **mandatory**      | the results you want to plot | *interrogator()` total |\n",
    " | *fract_of* | None      | results for plotting relative frequencies/ratios etc. | list (interrogator('c') form) |\n",
    " | *num_to_plot* | 7     | number of top results to display     |   integer |\n",
    " | *sort_by* | 'increase'/'decrease'/'static'/'total'     | show results increasing or decreasing the most in frequency  |   string |\n",
    " | *skip63* | False    | do not plot 1963     |    integer |\n",
    " | *proj63* | 4     | multiplier to project 1963 results and totals | integer |\n",
    " | *multiplier* | 100     | result * multiplier / total: use 1 for ratios | integer |\n",
    " | *x_label* | False    | custom label for the x-axis     |  string |\n",
    " | *y_label* | False    | custom label for the y-axis     |  string |\n",
    " | *legend_totals* | False    | Print total/rel freq in legend     |  boolean \n",
    " | *legend_p* | False    | Print p value of increase/decrease slope in legend   |  boolean \n",
    " | *projection* | True    | Project 1963 and 2014 editions     |  boolean |\n",
    " | *yearspan* | False    | plot a span of years |  a list of two int years |\n",
    " | *csvmake* | False    | make csvmake the title of csv output file    |  string |\n",
    " | *save* | False    | save generated image (True = with title as name)   |  True/False/string |\n",
    "\n",
    "You can easily use these to get different kinds of output. Try changing some parameters below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotter('Relative frequencies of risk words', riskwords.results, fract_of = allwords.totals,\n",
    "    y_label = 'Percentage of all risk words', num_to_plot = 5, \n",
    "    skip63 = False, projection = True, proj63 = 5, csvmake = 'riskwords.csv', legend_totals = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just generated a csv file, you can quickly get the results with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat 'riskwords.csv'  | head -n 7\n",
    "# and to delete it:\n",
    "#!rm 'riskwords.csv'\n",
    "\n",
    "# Use *yearspan* or *justyears* to specify years of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotter('Relative frequencies of risk words', riskwords.results, fract_of = allwords.totals,\n",
    "    y_label = 'Percentage of all risk words', num_to_plot = 5, skip63 = False, \n",
    "    yearspan = [1963,1998])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to change `plotter()` visualisations is by not passing certain results to `plotter()`.\n",
    "\n",
    "Each entry in the list of results is indexed: the top result is item 0, the second result is item 1, and so on.\n",
    "\n",
    "So, you can skip the first 2 results by using [2:] after the results list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotter('Relative frequencies of risk words', riskwords.results[2:], fract_of = allwords.totals,\n",
    "    y_label = 'Percentage of all risk words', num_to_plot = 5, skip63 = False, projection = True, proj63 = 5, legend_totals = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are after a specific set of indexed items, it's probably better to use `surgeon()` (described below). For completeness, though, here's another way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices_we_want = [32,30,40]\n",
    "plotter('Relative frequencies of risk words', [ riskwords.results[i] for i in indices_we_want], \n",
    "        num_to_plot = 5, skip63 = True, projection = True, proj63 = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another neat thing you can do is save the results of an interrogation, so they don't have to be run the next time you load this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# specify what to save, and a name for the file.\n",
    "from corpkit import save_result, load_result\n",
    "save_result(allwords, 'allwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then load these results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fromfile_allwords = load_result('allwords')\n",
    "fromfile_allwords.totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to quickly table the results of a csv file, you can use `table()`. Its only main argument is the path to the csv file as string. There are two optional arguments. First, you can set `allresults` to `True` to table all results, rather than just the plotted results. When this option is set to true, you may get *way* too many results. To cope with this, there is a `maxresults` argument, whose value by default is 50. You can overwrite this default to table more or fewer results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table('riskwords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table('riskwords.csv', allresults = True, maxresults = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quickview()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`quickview()` is a function that quickly shows the n most frequent items in a list. Its arguments are:\n",
    "\n",
    "1. an `interrogator()` result\n",
    "2. number of results to show (default = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quickview(riskwords.results, n = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number shown next to the item is its index. You can use this number to refer to an entry when editing results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tally()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tally()` simply displays the total occurrences of results. Its first argument is the list you want tallies from. For its second argument, you can use:\n",
    "\n",
    "* a list of indices for results you want to tally\n",
    "* a single integer, which will be interpreted as the index of the item you want\n",
    "* a regular expression to search for\n",
    "* a string, 'all', which will tally every result. This could be very many results, so it may be worth limiting the number of items you pass to it with [:n], as in the second example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tally(riskwords.results, [0, 5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tally(riskwords.results[:10], 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Regular Expression option is useful for merging results (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### surgeon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results lists can be edited quickly with `surgeon()`. `surgeon()`'s arguments are:\n",
    "\n",
    "1. an `interrogator()` results list\n",
    "2. *criteria*: either a [Regular Expression](http://www.cheatography.com/davechild/cheat-sheets/regular-expressions/) or a list of indices.\n",
    "3. *remove = True/False*\n",
    "\n",
    "By default, `surgeon()` keeps anything matching the regex, but this can be inverted with a *remove = True* argument. Because you are duplicating the original list, you don't have to worry about deleting `interrogator()` results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# low and high risks, using indices \n",
    "lowhighrisks = surgeon(riskwords.results, [4, 9, 17]) # keep 4, 9 and 17\n",
    "plotter('Low-, high- and higher- risk', lowhighrisks.results, num_to_plot = 3, skip63 = True)\n",
    "\n",
    "# only hyphenate words:\n",
    "nohyphenates = surgeon(riskwords.results, r'\\b.*-.*\\b', remove = True) # remove tokens with hyphens\n",
    "quickview(nohyphenates.results)\n",
    "plotter('Non-hypenate risk words', nohyphenates.results, fract_of = riskwords.totals, \n",
    "    y_label = 'Percentage of all risk words', num_to_plot = 7, skip63 = True)\n",
    "\n",
    "# only verbal risk words\n",
    "verbalrisks = surgeon(riskwords.results, r'^\\(v.*') #keep any token with tag starting with 'v'\n",
    "plotter('Verbal risk words', verbalrisks, fract_of = allwords.totals, \n",
    "    y_label = 'Percentage of all words', num_to_plot = 6, skip63 = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the warning you'll receive if you specify an interrogation, rather than a results list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`merger()` is for merging items in a list. Like `surgeon()`, it duplicates the old list. Its arguments are:\n",
    "\n",
    "1. the list you want to modify\n",
    "2. the indices of results you want to merge, or a regex to match\n",
    "3. newname = *str/int/False*: \n",
    "  * if string, the string becomes the merged item name.\n",
    "  * if integer, the merged entry takes the name of the item indexed with the integer.\n",
    "  * if not specified/False, the most most frequent item in the list becomes the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "low_high_combined = merger(lowhighrisks.results, [0, 2],  newname = 'high/higher risk')\n",
    "plotter('Low and high risks', low_high_combined.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diversity of risk words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that the kind of results we generate are hackable. Using some straight Python, combined with `merger()`, we can figure out how unique risk words appear in the NYT each year.\n",
    "\n",
    "To do this, we can take `riskwords.results`, duplicate it, and change every count over 0 into 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy # let's not modify our old list\n",
    "all_ones = copy.deepcopy(riskwords.results)\n",
    "for entry in all_ones:\n",
    "    for tup in entry[1:]:\n",
    "        if tup[1] > 0:\n",
    "            tup[1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use `merger()` to merge every entry. This will tell use how many unique words there are each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this generates heaps of output, so let's clear it\n",
    "mergedresults = merger(all_ones.results, r'.*', newname = 'Different risk words')\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# you could also use mergedresults.results[0]\n",
    "plotter('Diversity of risk words', mergedresults.totals, \n",
    "    skip63 = True, y_label = 'Unique risk words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see a generally upward trajectory, with more risk words constantly being used. Many of these results appear once, however, and many are nonwords. *Can you figure out how to remove words that appear only once per year?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`conc()` produces concordances of a subcorpus based on a Tregex query. Its main arguments are:\n",
    "\n",
    "1. A subcorpus to search *(remember to put it in quotation marks!)*\n",
    "2. A Tregex query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here, we use a subcorpus of politics articles,\n",
    "# rather than the total annual editions.\n",
    "lines = conc('data/nyt/trees/politics/1999', r'/JJ.?/ << /(?i).?\\brisk.?\\b/') # adj containing a risk word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set `conc()` to print *n* random concordances with the *random = n* parameter. You can also store the output to a variable for further searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = randoms = conc('data/nyt/trees/years/2007', r'/VB.?/ < /(?i).?\\brisk.?\\b/', random = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`conc()` takes another argument, window, which alters the amount of co-text appearing either side of the match. The default is 50 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = conc('data/nyt/trees/health/2013', r'/VB.?/ << /(?i).?\\brisk.?\\b/', random = 25, window = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`conc()` also allows you to view parse trees. By default, it's false:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = conc('data/nyt/trees/years/2013', r'/VB.?/ < /(?i)\\btrad.?/', trees = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final `conc()` argument is a *csv = 'filename'*, which will produce a tab-separated spreadsheet with the results of your query. You can copy and paste this data into Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = conc('data/nyt/trees/years/2005', r'/JJ.?/ < /(?i).?\\brisk.?/ > (NP <<# /(?i)invest.?/)',\n",
    "    window = 30, trees = False, csvmake = 'concordances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! cat 'concordances.csv'\n",
    "# and to delete it:\n",
    "# ! rm 'concordances.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords, ngrams and collocates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also functions for keywording, ngramming and collocation. Each can take a number of kinds of input data:\n",
    "\n",
    "1. a path to a subcorpus (of either parse trees or raw text)\n",
    "2. a path to a csv file generated with `conc()`\n",
    "3. a string of text\n",
    "4. a list of strings (i.e. output from `conc()`) \n",
    "\n",
    "`keywords()` produces both keywords and ngrams. It relies on code from the [Spindle](http://openspires.oucs.ox.ac.uk/spindle/) project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys, ngrams = keywords('concordances.csv')\n",
    "for key in keys[:10]:\n",
    "    print key\n",
    "for ngram in ngrams:\n",
    "    print ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even use `interrogator()` to get keywords or ngrams from each subcorpus. To do this, use 'keywords' or 'ngrams' as the query argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kwds = interrogator(annual_trees, 't', 'keywords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotter('Keywords', kwds.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `collocates()` function, you can specify the maximum distance at which two tokens will be considered collocates. The default is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colls = collocates('data/nyt/years/1989')\n",
    "for coll in colls:\n",
    "    print coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colls = collocates('concordances.csv', window = 2)\n",
    "for coll in colls:\n",
    "    print coll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quicktree() and searchtree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two functions are useful for visualising and searching individual syntax trees. They might be useful if you want to practive your Tregex queries, or make sure you are getting the expected result.\n",
    "\n",
    "The easiest place to get a parse tree is from a CSV file generated using `conc()` with *trees* set to *True*. Alternatively, you can open files in the data directory directly.\n",
    "\n",
    "`quicktree()` generates a visual representation of a parse tree. Here's one from 1989:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree = '(ROOT (S (NP (NN Pre-conviction) (NN attachment)) (VP (VBZ carries) (PP (IN with) (NP (PRP it))) (NP (NP (DT the) (JJ obvious) (NN risk)) (PP (IN of) (S (VP (VBG imposing) (NP (JJ drastic) (NN punishment)) (PP (IN before) (NP (NN conviction)))))))) (. .)))'\n",
    "quicktree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`searchtree()` requires a tree and a Tregex query. It will return a list of query matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print searchtree(tree, r'/VB.?/ >># (VP $ NP)')\n",
    "print searchtree(tree, r'NP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systemic functional linguistics argues that the main verb (the *process*) can be one of a few types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dictionaries.process_types import processes\n",
    "print processes.relational\n",
    "print processes.verbal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these in our Tregex queries to look for the kinds of processes participant risks are involved in. First, let's get a count for all processes with risk participants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = r'/VB.?/ < /%s/ ># (VP ( < (NP <<# /(?i).?\\brisk.?/)))' % processes.relational\n",
    "relationals = interrogator(annual_trees, '-t', query, lemmatise = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotter('Relational processes', relationals.results, fract_of = proc_w_risk_part.totals)\n",
    "\n",
    "# > You can also use the process types as regexes in `merger()` and `surgeon()` to merge/remove results entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proper nouns and risk sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We searched to find the most common proper noun strings.\n",
    "\n",
    "`interrogator()`'s *titlefilter* option removes common titles, first names and determiners to make for more accurate counts. It is useful when the results being returned are groups/phrases, rather than single words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Most common proper noun phrases\n",
    "query = r'NP <# NNP >> (ROOT << /(?i).?\\brisk.?\\b/)'\n",
    "propernouns = interrogator(annual_trees, '-t', query, \n",
    "    titlefilter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotter('Most common proper noun phrases', propernouns.results, fract_of = propernouns.totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quickview(propernouns.results, n = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are a few entries here that refer to the same group. (f.d.a and food and drug administration, for example). We can use `merger()` to fix these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# indices change after merger, remember, so\n",
    "# make sure you quickview results after every merge.\n",
    "merged_propernouns = merger(propernouns.results, [13, 20])\n",
    "merged_propernouns = merger(merged_propernouns, [8, 32])\n",
    "merged_propernouns = merger(merged_propernouns, [42, 107])\n",
    "merged_propernouns = merger(merged_propernouns, [60, 111])\n",
    "merged_propernouns = merger(merged_propernouns, [183, 197])\n",
    "merged_propernouns = merger(merged_propernouns, [65, 127])\n",
    "merged_propernouns = merger(merged_propernouns, [84, 149], newname = 149)\n",
    "merged_propernouns = merger(merged_propernouns, [23, 130])\n",
    "quickview(merged_propernouns, n = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've merged some common results, we can use `surgeon()` to build some basic thematic categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make some new thematic lists\n",
    "people = surgeon(merged_propernouns, r'(?i)^\\b(bush|clinton|obama|greenspan|gore|johnson|mccain|romney'\n",
    "    r'|kennedy|giuliani|reagan)$\\b')\n",
    "nations = surgeon(merged_propernouns, r'(?i)^\\b(iraq|china|america|israel|russia|japan|frace|germany|iran\\\n",
    "|britain|u\\.s\\.|afghanistan|australia|canada|spain|mexico|pakistan|soviet union|india)$\\b')\n",
    "geopol = surgeon(merged_propernouns, r'(?i)^\\b(middle east|asia|europe|america|soviet union|european union)$\\b')\n",
    "#usplaces = surgeon(merged_propernouns, r'(?i)^\\b(new york|washington|wall street|california|manhattan\\\n",
    "#|new york city|new jersey|north korea|italy|greece|bosniaboston|los angeles|broadway|texas)$\\b',\\)\n",
    "companies = surgeon(merged_propernouns, r'(?i)^\\b(merck|avandia\\\n",
    "|citigroup|pfizer|bayer|enron|apple|microsoft|empire)$\\b')\n",
    "organisations = surgeon(merged_propernouns, r'(?i)^\\b((white house|congress|federal reserve|nasa|pentagon)\\b|'\n",
    "    r'f\\.d\\.a\\.|c\\.i\\.a\\.|f\\.b\\.i\\.|e\\.p\\.a\\.)$')\n",
    "medical = surgeon(merged_propernouns, r'(?i)^\\b(vioxx|aids|celebrex|f.d.a)\\b')\n",
    "# geopol[5][0] == u'e.u.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot some results\n",
    "plotter('People', people, fract_of = propernouns.totals, \n",
    "        y_label = 'Percentage of all proper noun groups', skip63 = True)\n",
    "\n",
    "plotter('Nations', nations, fract_of = propernouns.totals, \n",
    "        y_label = 'Percentage of all proper noun groups', skip63 = True)\n",
    "\n",
    "plotter('Geopolitical entities', geopol, fract_of = propernouns.totals,  \n",
    "        y_label = 'Percentage of all proper noun groups', skip63 = False)\n",
    "\n",
    "plotter('Companies', companies, fract_of = propernouns.totals, \n",
    "        y_label = 'Percentage of all proper noun groups', skip63 = True)\n",
    "\n",
    "plotter('Organisations', organisations, fract_of = propernouns.totals, \n",
    "        y_label = 'Percentage of all proper noun groups', skip63 = True)\n",
    "\n",
    "plotter('Medicine', medical, fract_of = propernouns.totals, num_to_plot = 4,\n",
    "        y_label = 'Percentage of all proper noun groups', skip63 = True, save = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These charts reveal some interesting patterns.\n",
    "\n",
    "* We can clearly see presidencies and rival candidates come and go\n",
    "* Similarly, the wars in Iraq and Afghanistan are easy to spot\n",
    "* Naturally, the Soviet Union is a very frequent topic in 1963. It rises in frequency until its collapse. More recently, Russia can be seen as more frequently co-occurring with risk words.\n",
    "* The Eurozone crisis is visible\n",
    "* From the Organisations and Things, we can see the appearance of Merck and Vioxx in 2004, as well as Empire..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vioxx = surgeon(propernouns.results, r'(?i)^\\b(vioxx|merck)\\b$')\n",
    "plotter('Merck and Vioxx', vioxx, fract_of = propernouns.totals, skip63 = True)\n",
    "plotter('Merck and Vioxx', vioxx, fract_of = propernouns.totals, yearspan = [1998,2012])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vioxx was removed from shelves following the discovery that it increased the risk of heart attack. It's interesting how even though terrorism and war may come to mind when thinking of *risk* in the past 15 years, this health topic is easily more prominent in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on out, it's up to you. Currently in development are resources for making parsed corpora, but really, that process only involves:\n",
    "\n",
    "1. Getting some plain text\n",
    "2. Parsing it with Stanford CoreNLP\n",
    "3. Putting the files in sequential subfolders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}